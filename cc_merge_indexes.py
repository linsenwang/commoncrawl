import json
import os
import requests
from tqdm import tqdm
import time
from requests.exceptions import ChunkedEncodingError, ConnectionError, ReadTimeout

# ========== é…ç½®åŒºåŸŸ ==========
DOMAIN = "theguardian.com/*"
OUTPUT = "guardian_index_merged.jsonl"
OUTPUT_DIR = "guardian_batches"  # æ‰¹æ¬¡æ–‡ä»¶ä¿å­˜ç›®å½•

INDEXES = ['CC-MAIN-2025-38', 'CC-MAIN-2025-33', 'CC-MAIN-2025-30', 'CC-MAIN-2025-26', 'CC-MAIN-2025-21', 'CC-MAIN-2025-18', 'CC-MAIN-2025-13', 'CC-MAIN-2025-08', 'CC-MAIN-2025-05', 'CC-MAIN-2024-51', 'CC-MAIN-2024-46', 'CC-MAIN-2024-42', 'CC-MAIN-2024-38', 'CC-MAIN-2024-33', 'CC-MAIN-2024-30', 'CC-MAIN-2024-26', 'CC-MAIN-2024-22', 'CC-MAIN-2024-18', 'CC-MAIN-2024-10', 'CC-MAIN-2023-50', 'CC-MAIN-2023-40', 'CC-MAIN-2023-23', 'CC-MAIN-2023-14', 'CC-MAIN-2023-06', 'CC-MAIN-2022-49', 'CC-MAIN-2022-40', 'CC-MAIN-2022-33', 'CC-MAIN-2022-27', 'CC-MAIN-2022-21', 'CC-MAIN-2022-05', 'CC-MAIN-2021-49', 'CC-MAIN-2021-43', 'CC-MAIN-2021-39', 'CC-MAIN-2021-31', 'CC-MAIN-2021-25', 'CC-MAIN-2021-21', 'CC-MAIN-2021-17', 'CC-MAIN-2021-10', 'CC-MAIN-2021-04', 'CC-MAIN-2020-50', 'CC-MAIN-2020-45', 'CC-MAIN-2020-40', 'CC-MAIN-2020-34', 'CC-MAIN-2020-29', 'CC-MAIN-2020-24', 'CC-MAIN-2020-16', 'CC-MAIN-2020-10', 'CC-MAIN-2020-05', 'CC-MAIN-2019-51', 'CC-MAIN-2019-47', 'CC-MAIN-2019-43', 'CC-MAIN-2019-39', 'CC-MAIN-2019-35', 'CC-MAIN-2019-30', 'CC-MAIN-2019-26', 'CC-MAIN-2019-22', 'CC-MAIN-2019-18', 'CC-MAIN-2019-13', 'CC-MAIN-2019-09', 'CC-MAIN-2019-04', 'CC-MAIN-2018-51', 'CC-MAIN-2018-47', 'CC-MAIN-2018-43', 'CC-MAIN-2018-39', 'CC-MAIN-2018-34', 'CC-MAIN-2018-30', 'CC-MAIN-2018-26', 'CC-MAIN-2018-22', 'CC-MAIN-2018-17', 'CC-MAIN-2018-13', 'CC-MAIN-2018-09', 'CC-MAIN-2018-05', 'CC-MAIN-2017-51', 'CC-MAIN-2017-47', 'CC-MAIN-2017-43', 'CC-MAIN-2017-39', 'CC-MAIN-2017-34', 'CC-MAIN-2017-30', 'CC-MAIN-2017-26', 'CC-MAIN-2017-22', 'CC-MAIN-2017-17', 'CC-MAIN-2017-13', 'CC-MAIN-2017-09', 'CC-MAIN-2017-04', 'CC-MAIN-2016-50', 'CC-MAIN-2016-44', 'CC-MAIN-2016-40', 'CC-MAIN-2016-36', 'CC-MAIN-2016-30', 'CC-MAIN-2016-26', 'CC-MAIN-2016-22', 'CC-MAIN-2016-18', 'CC-MAIN-2016-07', 'CC-MAIN-2015-48', 'CC-MAIN-2015-40', 'CC-MAIN-2015-35', 'CC-MAIN-2015-32', 'CC-MAIN-2015-27', 'CC-MAIN-2015-22', 'CC-MAIN-2015-18', 'CC-MAIN-2015-14', 'CC-MAIN-2015-11', 'CC-MAIN-2015-06', 'CC-MAIN-2014-52', 'CC-MAIN-2014-49', 'CC-MAIN-2014-42', 'CC-MAIN-2014-41', 'CC-MAIN-2014-35', 'CC-MAIN-2014-23', 'CC-MAIN-2014-15', 'CC-MAIN-2014-10', 'CC-MAIN-2013-48', 'CC-MAIN-2013-20', 'CC-MAIN-2012', 'CC-MAIN-2009-2010', 'CC-MAIN-2008-2009']

# ========== å‡½æ•°éƒ¨åˆ† ==========
def fetch_from_index(index_name, domain, retries=5, backoff=3):
    """
    ä» Common Crawl Index æŸ¥è¯¢æŒ‡å®šåŸŸåçš„è®°å½•ï¼Œå¸¦è‡ªåŠ¨é‡è¯•ä¸åˆ†é¡µåŠŸèƒ½ã€‚
    å¦‚æœä»»ä½•ä¸€ä¸ªåˆ†é¡µä¸‹è½½å¤±è´¥ï¼Œåˆ™æ•´ä¸ªç´¢å¼•çš„ä¸‹è½½ä»»åŠ¡å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    base_url = f"https://index.commoncrawl.org/{index_name}-index?url={domain}&output=json"
    
    # 1. æŸ¥è¯¢æ€»é¡µæ•°
    num_pages = 1
    try:
        page_check_url = f"{base_url}&showNumPages=true"
        print(f"æ­£åœ¨æŸ¥è¯¢ {index_name} çš„æ€»é¡µæ•°...")
        resp = requests.get(page_check_url, timeout=60)
        if resp.status_code == 200:
            page_info = json.loads(resp.text.strip().splitlines()[0])
            num_pages = page_info.get("pages", 1)
            print(f"  -> {index_name} å…±æœ‰ {num_pages} é¡µ")
        else:
            print(f"âš ï¸ æ— æ³•è·å– {index_name} çš„æ€»é¡µæ•°ï¼Œå°†åªå°è¯•æŠ“å–ç¬¬1é¡µã€‚çŠ¶æ€ç : {resp.status_code}")
    except Exception as e:
        print(f"âš ï¸ æŸ¥è¯¢æ€»é¡µæ•°æ—¶å‡ºé”™: {e}ï¼Œå°†åªå°è¯•æŠ“å–ç¬¬1é¡µã€‚")

    all_records = []
    # 2. å¾ªç¯æŠ“å–æ¯ä¸€é¡µ
    page_iterator = tqdm(range(num_pages), desc=f"  æŠ“å–é¡µé¢", unit="é¡µ")
    for page in page_iterator:
        page_url = f"{base_url}&page={page}"
        
        for attempt in range(1, retries + 1):
            try:
                with requests.get(page_url, stream=True, timeout=120) as resp:
                    if resp.status_code == 200:
                        lines = []
                        for line in resp.iter_lines():
                            if line:
                                try:
                                    lines.append(json.loads(line.decode('utf-8')))
                                except json.JSONDecodeError:
                                    continue
                        all_records.extend(lines)
                        break # å½“å‰é¡µæˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    else:
                        print(f"âŒ è®¿é—® {index_name} (é¡µ {page}) å¤±è´¥: HTTP {resp.status_code}")
                        if attempt == retries: # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•
                             print(f"âŒ ç´¢å¼• {index_name} ä¸‹è½½å¤±è´¥ï¼ˆé¡µé¢ {page} å¤šæ¬¡å°è¯•åä»å¤±è´¥ï¼‰ã€‚")
                             return [] # ã€ä¿®æ”¹ç‚¹ã€‘è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ•´ä½“å¤±è´¥
                        time.sleep(backoff * attempt)

            except (ChunkedEncodingError, ConnectionError, ReadTimeout) as e:
                page_iterator.set_postfix_str(f"ç¬¬ {attempt}/{retries} æ¬¡è¯·æ±‚å¤±è´¥: {e}", refresh=True)
                if attempt < retries:
                    wait = backoff * attempt
                    time.sleep(wait)
                else:
                    print(f"âŒ æ”¾å¼ƒ {index_name} (é¡µ {page})ï¼ˆå¤šæ¬¡ç½‘ç»œå¤±è´¥ï¼‰")
                    print(f"âŒ ç´¢å¼• {index_name} ä¸‹è½½å¤±è´¥ï¼ˆé¡µé¢ {page} å¤šæ¬¡å°è¯•åä»å¤±è´¥ï¼‰ã€‚")
                    return [] # ã€ä¿®æ”¹ç‚¹ã€‘è¿”å›ç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºæ•´ä½“å¤±è´¥
        
        time.sleep(0.5) # å¯¹APIå‹å¥½ï¼Œæ¯æ¬¡è¯·æ±‚åç¨ä½œåœé¡¿

    return all_records # åªæœ‰å½“æ‰€æœ‰é¡µé¢éƒ½æˆåŠŸæ—¶ï¼Œæ‰ä¼šè¿”å›å®Œæ•´è®°å½•


# ========== ä¸»æµç¨‹ ==========
os.makedirs(OUTPUT_DIR, exist_ok=True)

for idx in INDEXES:
    batch_path = os.path.join(OUTPUT_DIR, f"guardian_{idx}.jsonl")
    if os.path.exists(batch_path):
        print(f"âœ… è·³è¿‡ {idx}ï¼ˆå·²å­˜åœ¨ï¼‰")
        continue

    print(f"\n===== å¼€å§‹å¤„ç†ç´¢å¼•: {idx} =====")
    recs = fetch_from_index(idx, DOMAIN)
    
    if not recs:
        # å¦‚æœè¿”å›çš„æ˜¯ç©ºåˆ—è¡¨ï¼Œè¯´æ˜ä¸‹è½½å¤±è´¥
        print(f"âŒ {idx} æœªæŠ“å–åˆ°ä»»ä½•è®°å½•æˆ–ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡ä¿å­˜ã€‚")
        # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ä»¥æ ‡è®°å°è¯•è¿‡ï¼Œé¿å…ä¸‹æ¬¡é‡å¤å¤±è´¥
        open(batch_path, 'w').close()
        continue

    print(f"  -> {idx} å…±æŠ“å– {len(recs)} æ¡è®°å½•")

    with open(batch_path, "w", encoding="utf-8") as f:
        for rec in recs:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    print(f"âœ… å·²ä¿å­˜åˆ° {batch_path}")


print("\nğŸ§© æ‰€æœ‰æ‰¹æ¬¡å·²æŠ“å–å¹¶åˆ†åˆ«ä¿å­˜ã€‚å¼€å§‹åˆå¹¶...")

# ========== åˆå¹¶ ==========
all_records = []
for fname in tqdm(os.listdir(OUTPUT_DIR), desc="åŠ è½½æ‰¹æ¬¡"):
    if not fname.endswith(".jsonl"):
        continue
    with open(os.path.join(OUTPUT_DIR, fname), "r", encoding="utf-8") as f:
        for line in f:
            try:
                all_records.append(json.loads(line))
            except json.JSONDecodeError:
                continue

print(f"å…±åŠ è½½ {len(all_records)} æ¡è®°å½•ï¼Œæ­£åœ¨å»é‡...")

# ========== å»é‡ ==========
# (å»é‡é€»è¾‘ä¿æŒä¸å˜)
from urllib.parse import urlparse

def normalize_url(url: str) -> str:
    """æ ‡å‡†åŒ– URLï¼šå»æ‰ http/httpsã€æœ«å°¾æ–œæ ã€www ç­‰"""
    try:
        parsed = urlparse(url)
        netloc = parsed.netloc.lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        path = parsed.path.rstrip("/")
        return f"{netloc}{path}"
    except:
        return url # å¦‚æœurlæ ¼å¼å¼‚å¸¸ï¼Œè¿”å›åŸæ ·

def choose_better_record(old, new):
    """æ¯”è¾ƒä¸¤æ¡è®°å½•ï¼Œè¿”å›è´¨é‡æ›´é«˜çš„ä¸€æ¡"""
    if old.get("status") != "200" and new.get("status") == "200": return new
    if old.get("status") == "200" and new.get("status") != "200": return old
    old_html = "html" in (old.get("mime-detected", "") or "").lower()
    new_html = "html" in (new.get("mime-detected", "") or "").lower()
    if old_html and not new_html: return old
    if not old_html and new_html: return new
    try:
        if int(new.get("length", 0)) > int(old.get("length", 0)): return new
    except (ValueError, TypeError): pass
    if new.get("timestamp", "") > old.get("timestamp", ""): return new
    return old

def deduplicate_records(records):
    unique = {}
    for rec in tqdm(records, desc="å»é‡ä¸­"):
        url = rec.get("url")
        if not url: continue
        key = normalize_url(url)
        if key in unique:
            unique[key] = choose_better_record(unique[key], rec)
        else:
            unique[key] = rec
    return list(unique.values())


merged = deduplicate_records(all_records)
print(f"âœ… å»é‡åå‰©ä½™ {len(merged)} æ¡å”¯ä¸€è®°å½•")

# ========== ä¿å­˜ ==========
with open(OUTPUT, "w", encoding="utf-8") as f:
    for rec in merged:
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")

print(f"\nâœ… å·²åˆå¹¶å¹¶ä¿å­˜åˆ° {OUTPUT}")
print("ä¸‹ä¸€æ­¥ï¼šå¯ä½¿ç”¨æ­£æ–‡ä¸‹è½½è„šæœ¬æå–ç½‘é¡µå†…å®¹ã€‚")