# 2_download_and_merge.py
import json
import os
import requests
import time
from tqdm import tqdm
from requests.exceptions import RequestException
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
import collections

# ========== é…ç½®åŒºåŸŸ ==========
# æ­¤å¤„é…ç½®åº”ä¸è„šæœ¬1ä¿æŒä¸€è‡´ï¼Œæˆ–åªä¿ç•™ä¸‹è½½å’Œè¾“å‡ºç›¸å…³é…ç½®
OUTPUT = "guardian_index_merged.jsonl"
OUTPUT_DIR = "guardian_batches"
TASKS_FILE = "tasks.jsonl"
COMPLETED_LOG_FILE = "completed_tasks.log"

MAX_WORKERS = 3
REQUEST_TIMEOUT = 10

# ========== ä¸‹è½½å‡½æ•° ==========

def fetch_page(session, task):
    """
    Fetches a single page of results.
    Returns a tuple: (task, list_of_records) on success,
                     (task, error_string) on failure.
    """
    page_url = task['url']
    retries = 5
    backoff = 3

    for attempt in range(1, retries + 1):
        try:
            with session.get(page_url, stream=True, timeout=REQUEST_TIMEOUT) as resp:
                if resp.status_code == 200:
                    lines = []
                    for line in resp.iter_lines():
                        if line:
                            try:
                                lines.append(json.loads(line.decode('utf-8')))
                            except json.JSONDecodeError:
                                continue
                    return (task, lines)
                else:
                    error_msg = f"HTTP {resp.status_code}"
                    if resp.status_code in [404, 400]:
                        return (task, f"Fatal error: {error_msg}")
                    time.sleep(backoff * attempt)
        except RequestException as e:
            error_msg = str(e)
            time.sleep(backoff * attempt)

    return (task, f"Failed after {retries} attempts: {error_msg}")


def main_downloader():
    """
    ä¸‹è½½å™¨ä¸»æµç¨‹ï¼Œå…·å¤‡æ–­ç‚¹ç»­ä¼ å’Œè‡ªåŠ¨é‡è¯•åŠŸèƒ½ã€‚
    """
    print("===== é˜¶æ®µ 2: æ‰§è¡Œä¸‹è½½ =====")
    
    # --- å®Œæ•´æ€§æ£€æµ‹ä¸ä»»åŠ¡åŠ è½½ ---
    if not os.path.exists(TASKS_FILE):
        print(f"âŒ é”™è¯¯: ä»»åŠ¡æ–‡ä»¶ '{TASKS_FILE}' ä¸å­˜åœ¨ã€‚")
        print("è¯·å…ˆè¿è¡Œ `1_create_tasks.py` æ¥ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ã€‚")
        return False # è¿”å›å¤±è´¥çŠ¶æ€

    print("æ­£åœ¨åŠ è½½ä»»åŠ¡åˆ—è¡¨å’Œå·²å®Œæˆè®°å½•...")
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆå”¯ä¸€ID
    all_tasks = {}
    with open(TASKS_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            task = json.loads(line)
            task_id = f"{task['index']}_{task['page']}"
            all_tasks[task_id] = task

    completed_tasks_ids = set()
    if os.path.exists(COMPLETED_LOG_FILE):
        with open(COMPLETED_LOG_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                completed_tasks_ids.add(line.strip())
    
    tasks_to_do = [task for task_id, task in all_tasks.items() if task_id not in completed_tasks_ids]

    if not tasks_to_do:
        print("âœ… æ‰€æœ‰ä»»åŠ¡å‡å·²ä¸‹è½½å®Œæˆï¼")
        return True # è¿”å›æˆåŠŸçŠ¶æ€

    print(f"å…± {len(all_tasks)} ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­ {len(completed_tasks_ids)} ä¸ªå·²å®Œæˆã€‚")
    print(f"æœ¬è½®éœ€è¦ä¸‹è½½ {len(tasks_to_do)} ä¸ªä»»åŠ¡é¡µã€‚")
    
    # --- æŒä¹…åŒ–ä¸‹è½½å¾ªç¯ ---
    adapter = requests.adapters.HTTPAdapter(pool_connections=MAX_WORKERS, pool_maxsize=MAX_WORKERS)
    session = requests.Session()
    session.mount('https://', adapter)
    
    attempt = 1
    backoff_time = 10

    with open(COMPLETED_LOG_FILE, "a", encoding="utf-8") as log_file:
        while tasks_to_do:
            print(f"\n--- ç¬¬ {attempt} è½®ä¸‹è½½å°è¯• ---")
            print(f"å¾…å¤„ç†é¡µé¢æ•°: {len(tasks_to_do)}")
            
            tasks_failed_this_run = []
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_task = {executor.submit(fetch_page, session, task): task for task in tasks_to_do}
                progress = tqdm(as_completed(future_to_task), total=len(tasks_to_do), desc=f"ä¸‹è½½ä¸­ (ç¬¬ {attempt} è½®)")
                
                records_to_save = collections.defaultdict(list)
                tasks_completed_this_batch = []

                for future in progress:
                    task_done, result = future.result()
                    if isinstance(result, list):
                        records_to_save[task_done['index']].extend(result)
                        task_id = f"{task_done['index']}_{task_done['page']}"
                        tasks_completed_this_batch.append(task_id)
                    else:
                        tasks_failed_this_run.append(task_done)
                        progress.set_postfix_str(f"å¤±è´¥+1 ({task_done['index']}:{task_done['page']})")
            
            if records_to_save:
                print(f"æœ¬è½®æˆåŠŸ {len(tasks_completed_this_batch)} é¡µï¼Œæ­£åœ¨å†™å…¥æ–‡ä»¶å¹¶è®°å½•è¿›åº¦...")
                for index_name, records in tqdm(records_to_save.items(), desc="ä¿å­˜æ•°æ®", leave=False):
                    batch_path = os.path.join(OUTPUT_DIR, f"guardian_{index_name}.jsonl")
                    with open(batch_path, "a", encoding="utf-8") as f:
                        for rec in records:
                            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                
                # å°†æœ¬æ‰¹æ¬¡å®Œæˆçš„ä»»åŠ¡IDå†™å…¥æ—¥å¿—
                for task_id in tasks_completed_this_batch:
                    log_file.write(task_id + "\n")
                log_file.flush()

            tasks_to_do = tasks_failed_this_run
            
            if tasks_to_do:
                print(f"âŒ {len(tasks_to_do)} ä¸ªé¡µé¢ä¸‹è½½å¤±è´¥ã€‚å°†åœ¨ {backoff_time} ç§’åé‡è¯•...")
                time.sleep(backoff_time)
                attempt += 1
                backoff_time = min(backoff_time * 1.5, 300)
            else:
                print("\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰é¡µé¢å‡å·²æˆåŠŸä¸‹è½½ï¼ğŸ‰ğŸ‰ğŸ‰")
                break
    
    return True # è¿”å›æˆåŠŸçŠ¶æ€

# ========== åˆå¹¶ä¸å»é‡å‡½æ•° (ä¿æŒä¸å˜) ==========
def normalize_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        netloc = parsed.netloc.lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        path = parsed.path.rstrip("/")
        return f"{netloc}{path}"
    except:
        return url

def choose_better_record(old, new):
    if old.get("status") != "200" and new.get("status") == "200": return new
    if old.get("status") == "200" and new.get("status") != "200": return old
    old_html = "html" in (old.get("mime-detected", "") or "").lower()
    new_html = "html" in (new.get("mime-detected", "") or "").lower()
    if old_html and not new_html: return old
    if not old_html and new_html: return new
    try:
        if int(new.get("length", 0)) > int(old.get("length", 0)): return new
    except (ValueError, TypeError): pass
    if new.get("timestamp", "") > old.get("timestamp", ""): return new
    return old

def deduplicate_records(records):
    unique = {}
    for rec in tqdm(records, desc="å»é‡ä¸­"):
        url = rec.get("url")
        if not url: continue
        key = normalize_url(url)
        if key in unique:
            unique[key] = choose_better_record(unique[key], rec)
        else:
            unique[key] = rec
    return list(unique.values())


def main_merge_and_deduplicate():
    print("\n===== é˜¶æ®µ 3: åˆå¹¶ä¸å»é‡ =====")

    all_records = []
    if not os.path.isdir(OUTPUT_DIR):
        print(f"è¾“å‡ºç›®å½• {OUTPUT_DIR} ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå¹¶ã€‚")
        return
        
    batch_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(".jsonl")]
    for fname in tqdm(batch_files, desc="åŠ è½½æ‰¹æ¬¡æ–‡ä»¶"):
        file_path = os.path.join(OUTPUT_DIR, fname)
        if os.path.getsize(file_path) > 0:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        all_records.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

    if not all_records:
        print("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è®°å½•ï¼Œç¨‹åºç»“æŸã€‚")
        return
        
    print(f"å…±åŠ è½½ {len(all_records)} æ¡è®°å½•ï¼Œæ­£åœ¨å»é‡...")

    merged = deduplicate_records(all_records)
    print(f"âœ… å»é‡åå‰©ä½™ {len(merged)} æ¡å”¯ä¸€è®°å½•")

    with open(OUTPUT, "w", encoding="utf-8") as f:
        for rec in merged:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    print(f"\nâœ… å·²åˆå¹¶å¹¶ä¿å­˜åˆ° {OUTPUT}")
    print("ä¸‹ä¸€æ­¥ï¼šå¯ä½¿ç”¨æ­£æ–‡ä¸‹è½½è„šæœ¬æå–ç½‘é¡µå†…å®¹ã€‚")


if __name__ == "__main__":
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # é˜¶æ®µäºŒï¼šæ‰§è¡Œä¸‹è½½å¾ªç¯
    download_successful = main_downloader()
    
    # é˜¶æ®µä¸‰ï¼šä»…åœ¨ä¸‹è½½æˆåŠŸåæ‰§è¡Œåˆå¹¶ä¸å»é‡
    if download_successful:
        main_merge_and_deduplicate()