# 2_download_and_merge.py
import json
import os
import requests
import time
from tqdm import tqdm
from requests.exceptions import RequestException
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse

# ========== é…ç½®åŒºåŸŸ ==========
# æ­¤å¤„é…ç½®åº”ä¸è„šæœ¬1ä¿æŒä¸€è‡´ï¼Œæˆ–åªä¿ç•™ä¸‹è½½å’Œè¾“å‡ºç›¸å…³é…ç½®
OUTPUT = "guardian_index_merged.jsonl"
OUTPUT_DIR = "guardian_batches"
TASKS_FILE = "tasks.jsonl"
COMPLETED_LOG_FILE = "completed_tasks.log"

MAX_WORKERS = 30
REQUEST_TIMEOUT = 120

# ========== ä¸‹è½½å‡½æ•° (æ— å˜åŠ¨) ==========

def fetch_page(session, task):
    """
    Fetches a single page of results.
    Returns a tuple: (task, list_of_records) on success,
                     (task, error_string) on failure.
    """
    page_url = task['url']
    retries = 5
    backoff = 3

    for attempt in range(1, retries + 1):
        try:
            with session.get(page_url, stream=True, timeout=REQUEST_TIMEOUT) as resp:
                if resp.status_code == 200:
                    lines = []
                    for line in resp.iter_lines():
                        if line:
                            try:
                                lines.append(json.loads(line.decode('utf-8')))
                            except json.JSONDecodeError:
                                continue
                    return (task, lines)
                else:
                    error_msg = f"HTTP {resp.status_code}"
                    if resp.status_code in [404, 400]:
                        return (task, f"Fatal error: {error_msg}")
                    time.sleep(backoff * attempt)
        except RequestException as e:
            error_msg = str(e)
            time.sleep(backoff * attempt)

    return (task, f"Failed after {retries} attempts: {error_msg}")


def main_downloader():
    """
    ä¸‹è½½å™¨ä¸»æµç¨‹ï¼Œå…·å¤‡æ–­ç‚¹ç»­ä¼ å’Œè‡ªåŠ¨é‡è¯•åŠŸèƒ½ã€‚
    ã€æ–°ã€‘æ¯ä¸ªé¡µé¢ä¸‹è½½æˆåŠŸåç«‹å³ä¿å­˜ä¸ºç‹¬ç«‹æ–‡ä»¶ã€‚
    ã€æ–°ã€‘é€šè¿‡æ‰«æå·²æœ‰æ–‡ä»¶å’Œæ—¥å¿—æ¥åˆ¤æ–­å®ŒæˆçŠ¶æ€ï¼Œå®ç°æ›´å¼ºçš„é²æ£’æ€§ã€‚
    """
    print("===== é˜¶æ®µ 2: æ‰§è¡Œä¸‹è½½ =====")
    
    # --- å®Œæ•´æ€§æ£€æµ‹ä¸ä»»åŠ¡åŠ è½½ ---
    if not os.path.exists(TASKS_FILE):
        print(f"âŒ é”™è¯¯: ä»»åŠ¡æ–‡ä»¶ '{TASKS_FILE}' ä¸å­˜åœ¨ã€‚")
        print("è¯·å…ˆè¿è¡Œ `1_create_tasks.py` æ¥ç”Ÿæˆä»»åŠ¡åˆ—è¡¨ã€‚")
        return False

    print("æ­£åœ¨åŠ è½½ä»»åŠ¡åˆ—è¡¨...")
    all_tasks = {}
    with open(TASKS_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            task = json.loads(line)
            task_id = f"{task['index']}_{task['page']}"
            all_tasks[task_id] = task
    
    # --- ã€æ–°ã€‘æ›´é²æ£’çš„å·²å®Œæˆä»»åŠ¡æ£€æµ‹ ---
    completed_tasks_ids = set()
    
    # 1. é€šè¿‡æ‰«æè¾“å‡ºç›®å½•ä¸‹çš„æ–‡ä»¶æ¥ç¡®å®šå·²å®Œæˆä»»åŠ¡
    print(f"æ­£åœ¨æ‰«æç›®å½• '{OUTPUT_DIR}' ä»¥æ£€æµ‹å·²ä¸‹è½½çš„æ–‡ä»¶...")
    if os.path.isdir(OUTPUT_DIR):
        for filename in os.listdir(OUTPUT_DIR):
            if filename.startswith("page_") and filename.endswith(".jsonl"):
                filepath = os.path.join(OUTPUT_DIR, filename)
                # ç¡®ä¿æ–‡ä»¶ä¸æ˜¯ç©ºçš„
                if os.path.getsize(filepath) > 0:
                    # ä»æ–‡ä»¶å 'page_{index}_{page}.jsonl' è§£æå‡º task_id
                    task_id = filename[5:-6] 
                    completed_tasks_ids.add(task_id)
    print(f"é€šè¿‡æ‰«ææ–‡ä»¶ï¼Œæ‰¾åˆ° {len(completed_tasks_ids)} ä¸ªå·²å®Œæˆçš„ä»»åŠ¡ã€‚")

    # 2. ä»æ—¥å¿—æ–‡ä»¶ä¸­è¡¥å……å·²å®Œæˆä»»åŠ¡è®°å½•ï¼ˆä½œä¸ºè¡¥å……å’Œå…¼å®¹ï¼‰
    if os.path.exists(COMPLETED_LOG_FILE):
        initial_count = len(completed_tasks_ids)
        with open(COMPLETED_LOG_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                completed_tasks_ids.add(line.strip())
        added_from_log = len(completed_tasks_ids) - initial_count
        if added_from_log > 0:
            print(f"ä»æ—¥å¿—æ–‡ä»¶ '{COMPLETED_LOG_FILE}' ä¸­é¢å¤–åŠ è½½äº† {added_from_log} æ¡å®Œæˆè®°å½•ã€‚")

    tasks_to_do = [task for task_id, task in all_tasks.items() if task_id not in completed_tasks_ids]

    if not tasks_to_do:
        print("âœ… æ‰€æœ‰ä»»åŠ¡å‡å·²ä¸‹è½½å®Œæˆï¼")
        return True

    print(f"å…± {len(all_tasks)} ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­ {len(completed_tasks_ids)} ä¸ªå·²å®Œæˆã€‚")
    print(f"æœ¬è½®éœ€è¦ä¸‹è½½ {len(tasks_to_do)} ä¸ªä»»åŠ¡é¡µã€‚")
    
    # --- æŒä¹…åŒ–ä¸‹è½½å¾ªç¯ (æ­¤éƒ¨åˆ†é€»è¾‘ä¸å˜) ---
    adapter = requests.adapters.HTTPAdapter(pool_connections=MAX_WORKERS, pool_maxsize=MAX_WORKERS)
    session = requests.Session()
    session.mount('https://', adapter)
    
    attempt = 1
    backoff_time = 10

    with open(COMPLETED_LOG_FILE, "a", encoding="utf-8") as log_file:
        while tasks_to_do:
            print(f"\n--- ç¬¬ {attempt} è½®ä¸‹è½½å°è¯• ---")
            print(f"å¾…å¤„ç†é¡µé¢æ•°: {len(tasks_to_do)}")
            
            tasks_failed_this_run = []
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_task = {executor.submit(fetch_page, session, task): task for task in tasks_to_do}
                progress = tqdm(as_completed(future_to_task), total=len(tasks_to_do), desc=f"ä¸‹è½½ä¸­ (ç¬¬ {attempt} è½®)")
                
                tasks_completed_this_batch = []

                for future in progress:
                    task_done, result = future.result()
                    task_id = f"{task_done['index']}_{task_done['page']}"
                    
                    if isinstance(result, list):
                        page_filename = f"page_{task_id}.jsonl"
                        page_filepath = os.path.join(OUTPUT_DIR, page_filename)
                        
                        try:
                            with open(page_filepath, "w", encoding="utf-8") as f:
                                for rec in result:
                                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
                            tasks_completed_this_batch.append(task_id)
                            progress.set_postfix_str(f"å·²ä¿å­˜ {page_filename}")
                        except IOError:
                            tasks_failed_this_run.append(task_done)
                            progress.set_postfix_str(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {page_filename}")
                    else:
                        tasks_failed_this_run.append(task_done)
                        progress.set_postfix_str(f"ä¸‹è½½å¤±è´¥: {task_id}")
            
            if tasks_completed_this_batch:
                print(f"æœ¬è½®æˆåŠŸ {len(tasks_completed_this_batch)} é¡µï¼Œæ­£åœ¨è®°å½•è¿›åº¦...")
                for task_id in tasks_completed_this_batch:
                    log_file.write(task_id + "\n")
                log_file.flush()

            tasks_to_do = tasks_failed_this_run
            
            if tasks_to_do:
                print(f"âŒ {len(tasks_to_do)} ä¸ªé¡µé¢å¤„ç†å¤±è´¥ã€‚å°†åœ¨ {backoff_time} ç§’åé‡è¯•...")
                time.sleep(backoff_time)
                attempt += 1
                backoff_time = min(backoff_time * 1.5, 300)
            else:
                print("\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰é¡µé¢å‡å·²æˆåŠŸä¸‹è½½ï¼ğŸ‰ğŸ‰ğŸ‰")
                break
    
    return True

# ========== åˆå¹¶ä¸å»é‡å‡½æ•° (æ— å˜åŠ¨) ==========
def normalize_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        netloc = parsed.netloc.lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        path = parsed.path.rstrip("/")
        return f"{netloc}{path}"
    except:
        return url

def choose_better_record(old, new):
    if old.get("status") != "200" and new.get("status") == "200": return new
    if old.get("status") == "200" and new.get("status") != "200": return old
    old_html = "html" in (old.get("mime-detected", "") or "").lower()
    new_html = "html" in (new.get("mime-detected", "") or "").lower()
    if old_html and not new_html: return old
    if not old_html and new_html: return new
    try:
        if int(new.get("length", 0)) > int(old.get("length", 0)): return new
    except (ValueError, TypeError): pass
    if new.get("timestamp", "") > old.get("timestamp", ""): return new
    return old

def deduplicate_records(records):
    unique = {}
    for rec in tqdm(records, desc="å»é‡ä¸­"):
        url = rec.get("url")
        if not url: continue
        key = normalize_url(url)
        if key in unique:
            unique[key] = choose_better_record(unique[key], rec)
        else:
            unique[key] = rec
    return list(unique.values())


def main_merge_and_deduplicate():
    print("\n===== é˜¶æ®µ 3: åˆå¹¶ä¸å»é‡ =====")

    all_records = []
    if not os.path.isdir(OUTPUT_DIR):
        print(f"è¾“å‡ºç›®å½• {OUTPUT_DIR} ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå¹¶ã€‚")
        return
        
    # è¯»å–æ‰€æœ‰ .jsonl æ–‡ä»¶ï¼Œä¸è®ºå…¶å‰ç¼€æ˜¯ä»€ä¹ˆ
    batch_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(".jsonl")]
    if not batch_files:
        print(f"åœ¨è¾“å‡ºç›®å½• '{OUTPUT_DIR}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .jsonl æ–‡ä»¶ï¼Œæ— æ³•åˆå¹¶ã€‚")
        return

    for fname in tqdm(batch_files, desc="åŠ è½½æ‰¹æ¬¡æ–‡ä»¶"):
        file_path = os.path.join(OUTPUT_DIR, fname)
        if os.path.getsize(file_path) > 0:
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        all_records.append(json.loads(line))
                    except json.JSONDecodeError:
                        continue

    if not all_records:
        print("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è®°å½•ï¼Œç¨‹åºç»“æŸã€‚")
        return
        
    print(f"å…±åŠ è½½ {len(all_records)} æ¡è®°å½•ï¼Œæ­£åœ¨å»é‡...")

    merged = deduplicate_records(all_records)
    print(f"âœ… å»é‡åå‰©ä½™ {len(merged)} æ¡å”¯ä¸€è®°å½•")

    with open(OUTPUT, "w", encoding="utf-8") as f:
        for rec in merged:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    print(f"\nâœ… å·²åˆå¹¶å¹¶ä¿å­˜åˆ° {OUTPUT}")
    print("ä¸‹ä¸€æ­¥ï¼šå¯ä½¿ç”¨æ­£æ–‡ä¸‹è½½è„šæœ¬æå–ç½‘é¡µå†…å®¹ã€‚")


if __name__ == "__main__":
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # é˜¶æ®µäºŒï¼šæ‰§è¡Œä¸‹è½½å¾ªç¯
    download_successful = main_downloader()
    
    # é˜¶æ®µä¸‰ï¼šä»…åœ¨ä¸‹è½½æˆåŠŸåæ‰§è¡Œåˆå¹¶ä¸å»é‡
    if download_successful:
        main_merge_and_deduplicate()