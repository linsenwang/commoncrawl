import os
from tqdm import tqdm
import json
from urllib.parse import urlparse

OUTPUT_DIR = "guardian_batches_part2"
OUTPUT = "guardian_index_part2.jsonl"
TEMP_MERGED = "guardian_index_partial.jsonl"
BATCH_SIZE = 100  # ä¸€æ¬¡å¤„ç†å¤šå°‘ä¸ªæ–‡ä»¶ï¼Œå¯æ ¹æ®å†…å­˜è°ƒæ•´

def normalize_url(url: str) -> str:
    try:
        parsed = urlparse(url)
        netloc = parsed.netloc.lower()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        path = parsed.path.rstrip("/")
        return f"{netloc}{path}"
    except Exception:
        return url

def choose_better_record(old, new):
    if old.get("status") != "200" and new.get("status") == "200":
        return new
    if old.get("status") == "200" and new.get("status") != "200":
        return old
    old_html = "html" in (old.get("mime-detected", "") or "").lower()
    new_html = "html" in (new.get("mime-detected", "") or "").lower()
    if old_html and not new_html:
        return old
    if not old_html and new_html:
        return new
    try:
        if int(new.get("length", 0)) > int(old.get("length", 0)):
            return new
    except (ValueError, TypeError):
        pass
    if new.get("timestamp", "") > old.get("timestamp", ""):
        return new
    return old

def deduplicate_records(records, existing_map=None):
    unique = existing_map or {}
    for rec in tqdm(records, desc="å»é‡ä¸­", leave=False):
        url = rec.get("url")
        if not url:
            continue
        key = normalize_url(url)
        if key in unique:
            unique[key] = choose_better_record(unique[key], rec)
        else:
            unique[key] = rec
    return unique

def load_jsonl_file(file_path):
    records = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                records.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return records

def save_jsonl(data_dict, output_path):
    with open(output_path, "w", encoding="utf-8") as f:
        for rec in data_dict.values():
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

def main_merge_and_deduplicate():
    print("\n===== é˜¶æ®µ 3: åˆ†æ‰¹åˆå¹¶ä¸å»é‡ =====")

    if not os.path.isdir(OUTPUT_DIR):
        print(f"è¾“å‡ºç›®å½• {OUTPUT_DIR} ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå¹¶ã€‚")
        return

    batch_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.endswith(".jsonl")])
    if not batch_files:
        print(f"åœ¨è¾“å‡ºç›®å½• '{OUTPUT_DIR}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .jsonl æ–‡ä»¶ã€‚")
        return

    global_unique = {}

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, len(batch_files), BATCH_SIZE):
        subset = batch_files[i:i + BATCH_SIZE]
        print(f"\nğŸ“¦ å¤„ç†ç¬¬ {i // BATCH_SIZE + 1} æ‰¹ï¼Œå…± {len(subset)} ä¸ªæ–‡ä»¶")
        all_records = []
        for fname in tqdm(subset, desc="åŠ è½½æ–‡ä»¶"):
            file_path = os.path.join(OUTPUT_DIR, fname)
            if os.path.getsize(file_path) == 0:
                continue
            all_records.extend(load_jsonl_file(file_path))

        print(f"â†’ å½“å‰æ‰¹æ¬¡åŠ è½½ {len(all_records)} æ¡è®°å½•ï¼Œæ­£åœ¨å»é‡...")
        global_unique = deduplicate_records(all_records, existing_map=global_unique)
        print(f"â†’ å½“å‰ç´¯è®¡å”¯ä¸€è®°å½•æ•°ï¼š{len(global_unique)}")

        # ä¸­é—´ä¿å­˜ä¸€æ¬¡ï¼Œé˜²æ­¢ä¸­é€”å´©æºƒä¸¢æ•°æ®
        save_jsonl(global_unique, TEMP_MERGED)
        print(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœè‡³ {TEMP_MERGED}")

    print("\nâœ… å…¨éƒ¨åˆ†æ‰¹å¤„ç†å®Œæ¯•ï¼Œæœ€ç»ˆä¿å­˜ç»“æœ...")
    save_jsonl(global_unique, OUTPUT)
    print(f"âœ… å·²åˆå¹¶å¹¶ä¿å­˜åˆ° {OUTPUT}")
    print("ä¸‹ä¸€æ­¥ï¼šå¯ä½¿ç”¨æ­£æ–‡ä¸‹è½½è„šæœ¬æå–ç½‘é¡µå†…å®¹ã€‚")

if __name__ == "__main__":
    main_merge_and_deduplicate()